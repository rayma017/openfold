{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env|grep OPEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "!env|grep DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install py3dmol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-basic",
   "metadata": {},
   "source": [
    "# Visualizing molecular structures with py3DMol\n",
    "\n",
    "In this computer lab, we will visualize the structures of various biomolecules using [py3DMol](https://github.com/avirshup/py3dmol). py3DMol which allows for dependency-free molecular visualization in Jupyter notebooks. py3DMol wraps the [3DMol.js](http://3dmol.csb.pitt.edu/doc/index.html) library for online molecular visualization.\n",
    "\n",
    "This lab is an abbreviated and adapted version of [Lab 02 of IIBM3202 Molecular Modeling and Simulation](https://github.com/pb3lab/ibm3202/blob/master/tutorials/lab02_molviz.ipynb) from the Institute for Biological and Engineering at Pontificia Universidad Catolica de Chile.\n",
    "\n",
    "As a first step, we will need to install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import py3Dmol\n",
    "except:\n",
    "  %conda install py3dmol\n",
    "  import py3Dmol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 AlQuraishi Laboratory\n",
    "# Copyright 2021 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__file__)\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "import torch\n",
    "torch_versions = torch.__version__.split(\".\")\n",
    "torch_major_version = int(torch_versions[0])\n",
    "torch_minor_version = int(torch_versions[1])\n",
    "if (\n",
    "    torch_major_version > 1 or\n",
    "    (torch_major_version == 1 and torch_minor_version >= 12)\n",
    "):\n",
    "    # Gives a large speedup on Ampere-class GPUs\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from openfold.config import model_config\n",
    "from openfold.data import templates, feature_pipeline, data_pipeline\n",
    "from openfold.data.tools import hhsearch, hmmsearch\n",
    "from openfold.np import protein\n",
    "from openfold.utils.script_utils import (load_models_from_command_line, parse_fasta, run_model,\n",
    "                                         prep_output, relax_protein)\n",
    "from openfold.utils.tensor_utils import tensor_tree_map\n",
    "from openfold.utils.trace_utils import (\n",
    "    pad_feature_dict_seq,\n",
    "    trace_model_,\n",
    ")\n",
    "\n",
    "from scripts.precompute_embeddings import EmbeddingGenerator\n",
    "from scripts.utils import add_data_args\n",
    "\n",
    "\n",
    "TRACING_INTERVAL = 50\n",
    "\n",
    "\n",
    "def precompute_alignments(tags, seqs, alignment_dir, args):\n",
    "    for tag, seq in zip(tags, seqs):\n",
    "        tmp_fasta_path = os.path.join(args.output_dir, f\"tmp_{os.getpid()}.fasta\")\n",
    "        with open(tmp_fasta_path, \"w\") as fp:\n",
    "            fp.write(f\">{tag}\\n{seq}\")\n",
    "\n",
    "        local_alignment_dir = os.path.join(alignment_dir, tag)\n",
    "\n",
    "        if args.use_precomputed_alignments is None:\n",
    "            logger.info(f\"Generating alignments for {tag}...\")\n",
    "\n",
    "            os.makedirs(local_alignment_dir, exist_ok=True)\n",
    "\n",
    "            if \"multimer\" in args.config_preset:\n",
    "                template_searcher = hmmsearch.Hmmsearch(\n",
    "                    binary_path=args.hmmsearch_binary_path,\n",
    "                    hmmbuild_binary_path=args.hmmbuild_binary_path,\n",
    "                    database_path=args.pdb_seqres_database_path,\n",
    "                )\n",
    "            else:\n",
    "                template_searcher = hhsearch.HHSearch(\n",
    "                    binary_path=args.hhsearch_binary_path,\n",
    "                    databases=[args.pdb70_database_path],\n",
    "                )\n",
    "\n",
    "            # In seqemb mode, use AlignmentRunner only to generate templates\n",
    "            if args.use_single_seq_mode:\n",
    "                alignment_runner = data_pipeline.AlignmentRunner(\n",
    "                    jackhmmer_binary_path=args.jackhmmer_binary_path,\n",
    "                    uniref90_database_path=args.uniref90_database_path,\n",
    "                    template_searcher=template_searcher,\n",
    "                    no_cpus=args.cpus,\n",
    "                )\n",
    "                embedding_generator = EmbeddingGenerator()\n",
    "                embedding_generator.run(tmp_fasta_path, alignment_dir)\n",
    "            else:\n",
    "                alignment_runner = data_pipeline.AlignmentRunner(\n",
    "                    jackhmmer_binary_path=args.jackhmmer_binary_path,\n",
    "                    hhblits_binary_path=args.hhblits_binary_path,\n",
    "                    uniref90_database_path=args.uniref90_database_path,\n",
    "                    mgnify_database_path=args.mgnify_database_path,\n",
    "                    bfd_database_path=args.bfd_database_path,\n",
    "                    uniref30_database_path=args.uniref30_database_path,\n",
    "                    uniclust30_database_path=args.uniclust30_database_path,\n",
    "                    uniprot_database_path=args.uniprot_database_path,\n",
    "                    template_searcher=template_searcher,\n",
    "                    use_small_bfd=args.bfd_database_path is None,\n",
    "                    no_cpus=args.cpus\n",
    "                )\n",
    "\n",
    "            alignment_runner.run(\n",
    "                tmp_fasta_path, local_alignment_dir\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"Using precomputed alignments for {tag} at {alignment_dir}...\"\n",
    "            )\n",
    "\n",
    "        # Remove temporary FASTA file\n",
    "        os.remove(tmp_fasta_path)\n",
    "\n",
    "\n",
    "def round_up_seqlen(seqlen):\n",
    "    return int(math.ceil(seqlen / TRACING_INTERVAL)) * TRACING_INTERVAL\n",
    "\n",
    "\n",
    "def generate_feature_dict(\n",
    "    tags,\n",
    "    seqs,\n",
    "    alignment_dir,\n",
    "    data_processor,\n",
    "    args,\n",
    "):\n",
    "    tmp_fasta_path = os.path.join(args.output_dir, f\"tmp_{os.getpid()}.fasta\")\n",
    "\n",
    "    if \"multimer\" in args.config_preset:\n",
    "        with open(tmp_fasta_path, \"w\") as fp:\n",
    "            fp.write(\n",
    "                '\\n'.join([f\">{tag}\\n{seq}\" for tag, seq in zip(tags, seqs)])\n",
    "            )\n",
    "        feature_dict = data_processor.process_fasta(\n",
    "            fasta_path=tmp_fasta_path, alignment_dir=alignment_dir,\n",
    "        )\n",
    "    elif len(seqs) == 1:\n",
    "        tag = tags[0]\n",
    "        seq = seqs[0]\n",
    "        with open(tmp_fasta_path, \"w\") as fp:\n",
    "            fp.write(f\">{tag}\\n{seq}\")\n",
    "\n",
    "        local_alignment_dir = os.path.join(alignment_dir, tag)\n",
    "        feature_dict = data_processor.process_fasta(\n",
    "            fasta_path=tmp_fasta_path,\n",
    "            alignment_dir=local_alignment_dir,\n",
    "            seqemb_mode=args.use_single_seq_mode,\n",
    "        )\n",
    "    else:\n",
    "        with open(tmp_fasta_path, \"w\") as fp:\n",
    "            fp.write(\n",
    "                '\\n'.join([f\">{tag}\\n{seq}\" for tag, seq in zip(tags, seqs)])\n",
    "            )\n",
    "        feature_dict = data_processor.process_multiseq_fasta(\n",
    "            fasta_path=tmp_fasta_path, super_alignment_dir=alignment_dir,\n",
    "        )\n",
    "\n",
    "    # Remove temporary FASTA file\n",
    "    os.remove(tmp_fasta_path)\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "\n",
    "def list_files_with_extensions(dir, extensions):\n",
    "    return [f for f in os.listdir(dir) if f.endswith(extensions)]\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Create the output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    if args.config_preset.startswith(\"seq\"):\n",
    "        args.use_single_seq_mode = True\n",
    "\n",
    "    config = model_config(\n",
    "        args.config_preset, \n",
    "        long_sequence_inference=args.long_sequence_inference,\n",
    "        use_deepspeed_evoformer_attention=args.use_deepspeed_evoformer_attention,\n",
    "        )\n",
    "\n",
    "    if args.experiment_config_json: \n",
    "        with open(args.experiment_config_json, 'r') as f:\n",
    "            custom_config_dict = json.load(f)\n",
    "        config.update_from_flattened_dict(custom_config_dict)\n",
    "\n",
    "    if args.experiment_config_json: \n",
    "        with open(args.experiment_config_json, 'r') as f:\n",
    "            custom_config_dict = json.load(f)\n",
    "        config.update_from_flattened_dict(custom_config_dict)\n",
    "\n",
    "    if args.trace_model:\n",
    "        if not config.data.predict.fixed_size:\n",
    "            raise ValueError(\n",
    "                \"Tracing requires that fixed_size mode be enabled in the config\"\n",
    "            )\n",
    "\n",
    "    is_multimer = \"multimer\" in args.config_preset\n",
    "\n",
    "    if is_multimer:\n",
    "        template_featurizer = templates.HmmsearchHitFeaturizer(\n",
    "            mmcif_dir=args.template_mmcif_dir,\n",
    "            max_template_date=args.max_template_date,\n",
    "            max_hits=config.data.predict.max_templates,\n",
    "            kalign_binary_path=args.kalign_binary_path,\n",
    "            release_dates_path=args.release_dates_path,\n",
    "            obsolete_pdbs_path=args.obsolete_pdbs_path\n",
    "        )\n",
    "    else:\n",
    "        template_featurizer = templates.HhsearchHitFeaturizer(\n",
    "            mmcif_dir=args.template_mmcif_dir,\n",
    "            max_template_date=args.max_template_date,\n",
    "            max_hits=config.data.predict.max_templates,\n",
    "            kalign_binary_path=args.kalign_binary_path,\n",
    "            release_dates_path=args.release_dates_path,\n",
    "            obsolete_pdbs_path=args.obsolete_pdbs_path\n",
    "        )\n",
    "\n",
    "    data_processor = data_pipeline.DataPipeline(\n",
    "        template_featurizer=template_featurizer,\n",
    "    )\n",
    "\n",
    "    if is_multimer:\n",
    "        data_processor = data_pipeline.DataPipelineMultimer(\n",
    "            monomer_data_pipeline=data_processor,\n",
    "        )\n",
    "\n",
    "    output_dir_base = args.output_dir\n",
    "    random_seed = args.data_random_seed\n",
    "    if random_seed is None:\n",
    "        random_seed = random.randrange(2 ** 32)\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed + 1)\n",
    "\n",
    "    feature_processor = feature_pipeline.FeaturePipeline(config.data)\n",
    "    if not os.path.exists(output_dir_base):\n",
    "        os.makedirs(output_dir_base)\n",
    "    if args.use_precomputed_alignments is None:\n",
    "        alignment_dir = os.path.join(output_dir_base, \"alignments\")\n",
    "    else:\n",
    "        alignment_dir = args.use_precomputed_alignments\n",
    "\n",
    "    tag_list = []\n",
    "    seq_list = []\n",
    "    for fasta_file in list_files_with_extensions(args.fasta_dir, (\".fasta\", \".fa\")):\n",
    "        # Gather input sequences\n",
    "        fasta_path = os.path.join(args.fasta_dir, fasta_file)\n",
    "        with open(fasta_path, \"r\") as fp:\n",
    "            data = fp.read()\n",
    "\n",
    "        tags, seqs = parse_fasta(data)\n",
    "\n",
    "        if not is_multimer and len(tags) != 1:\n",
    "            print(\n",
    "                f\"{fasta_path} contains more than one sequence but \"\n",
    "                f\"multimer mode is not enabled. Skipping...\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # assert len(tags) == len(set(tags)), \"All FASTA tags must be unique\"\n",
    "        tag = '-'.join(tags)\n",
    "\n",
    "        tag_list.append((tag, tags))\n",
    "        seq_list.append(seqs)\n",
    "\n",
    "    seq_sort_fn = lambda target: sum([len(s) for s in target[1]])\n",
    "    sorted_targets = sorted(zip(tag_list, seq_list), key=seq_sort_fn)\n",
    "    feature_dicts = {}\n",
    "    model_generator = load_models_from_command_line(\n",
    "        config,\n",
    "        args.model_device,\n",
    "        args.openfold_checkpoint_path,\n",
    "        args.jax_param_path,\n",
    "        args.output_dir)\n",
    "\n",
    "    for model, output_directory in model_generator:\n",
    "        cur_tracing_interval = 0\n",
    "        for (tag, tags), seqs in sorted_targets:\n",
    "            output_name = f'{tag}_{args.config_preset}'\n",
    "            if args.output_postfix is not None:\n",
    "                output_name = f'{output_name}_{args.output_postfix}'\n",
    "\n",
    "            # Does nothing if the alignments have already been computed\n",
    "            precompute_alignments(tags, seqs, alignment_dir, args)\n",
    "\n",
    "            feature_dict = feature_dicts.get(tag, None)\n",
    "            if feature_dict is None:\n",
    "                feature_dict = generate_feature_dict(\n",
    "                    tags,\n",
    "                    seqs,\n",
    "                    alignment_dir,\n",
    "                    data_processor,\n",
    "                    args,\n",
    "                )\n",
    "\n",
    "                if args.trace_model:\n",
    "                    n = feature_dict[\"aatype\"].shape[-2]\n",
    "                    rounded_seqlen = round_up_seqlen(n)\n",
    "                    feature_dict = pad_feature_dict_seq(\n",
    "                        feature_dict, rounded_seqlen,\n",
    "                    )\n",
    "\n",
    "                feature_dicts[tag] = feature_dict\n",
    "\n",
    "            processed_feature_dict = feature_processor.process_features(\n",
    "                feature_dict, mode='predict', is_multimer=is_multimer\n",
    "            )\n",
    "\n",
    "            processed_feature_dict = {\n",
    "                k: torch.as_tensor(v, device=args.model_device)\n",
    "                for k, v in processed_feature_dict.items()\n",
    "            }\n",
    "\n",
    "            if args.trace_model:\n",
    "                if rounded_seqlen > cur_tracing_interval:\n",
    "                    logger.info(\n",
    "                        f\"Tracing model at {rounded_seqlen} residues...\"\n",
    "                    )\n",
    "                    t = time.perf_counter()\n",
    "                    trace_model_(model, processed_feature_dict)\n",
    "                    tracing_time = time.perf_counter() - t\n",
    "                    logger.info(\n",
    "                        f\"Tracing time: {tracing_time}\"\n",
    "                    )\n",
    "                    cur_tracing_interval = rounded_seqlen\n",
    "\n",
    "            out = run_model(model, processed_feature_dict, tag, args.output_dir)\n",
    "\n",
    "            # Toss out the recycling dimensions --- we don't need them anymore\n",
    "            processed_feature_dict = tensor_tree_map(\n",
    "                lambda x: np.array(x[..., -1].cpu()),\n",
    "                processed_feature_dict\n",
    "            )\n",
    "            out = tensor_tree_map(lambda x: np.array(x.cpu()), out)\n",
    "\n",
    "            unrelaxed_protein = prep_output(\n",
    "                out,\n",
    "                processed_feature_dict,\n",
    "                feature_dict,\n",
    "                feature_processor,\n",
    "                args.config_preset,\n",
    "                args.multimer_ri_gap,\n",
    "                args.subtract_plddt\n",
    "            )\n",
    "\n",
    "            unrelaxed_file_suffix = \"_unrelaxed.pdb\"\n",
    "            if args.cif_output:\n",
    "                unrelaxed_file_suffix = \"_unrelaxed.cif\"\n",
    "            unrelaxed_output_path = os.path.join(\n",
    "                output_directory, f'{output_name}{unrelaxed_file_suffix}'\n",
    "            )\n",
    "\n",
    "            with open(unrelaxed_output_path, 'w') as fp:\n",
    "                if args.cif_output:\n",
    "                    fp.write(protein.to_modelcif(unrelaxed_protein))\n",
    "                else:\n",
    "                    fp.write(protein.to_pdb(unrelaxed_protein))\n",
    "\n",
    "            logger.info(f\"Output written to {unrelaxed_output_path}...\")\n",
    "\n",
    "            if not args.skip_relaxation:\n",
    "                # Relax the prediction.\n",
    "                logger.info(f\"Running relaxation on {unrelaxed_output_path}...\")\n",
    "                relax_protein(config, args.model_device, unrelaxed_protein, output_directory, output_name,\n",
    "                              args.cif_output)\n",
    "\n",
    "            if args.save_outputs:\n",
    "                output_dict_path = os.path.join(\n",
    "                    output_directory, f'{output_name}_output_dict.pkl'\n",
    "                )\n",
    "                with open(output_dict_path, \"wb\") as fp:\n",
    "                    pickle.dump(out, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                logger.info(f\"Model output written to {output_dict_path}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-morocco",
   "metadata": {},
   "source": [
    "## Retrieving structures\n",
    "\n",
    "Now let us look at the structure of an important SARS-CoV-2 drug target, the main protease (MPro). Many structures of this enzyme are available on the [Protein Data Bank](https://www.rcsb.org/), including [5RH2](https://www.rcsb.org/structure/5RH2). This particular structure [helped inspire a clinical candidate](https://doi.org/10.1101/2022.01.26.477782) for COVID-19 treatment.\n",
    "\n",
    "<p align = \"justify\">You can retrieve PDB structures from its website (www.rcsb.org). Alternatively, you can directly use the terminal to download a given PDB file with known accession code as shown below, where XXXX must be replaced by the replaced by the 4-letter PDB code:\n",
    "\n",
    "```\n",
    "%%bash\n",
    "wget http://www.rcsb.org/pdb/files/XXXX.pdb.gz \n",
    "gunzip XXXX.pdb.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://www.rcsb.org/pdb/files/5RH2.pdb.gz \n",
    "gunzip 5RH2.pdb.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-thousand",
   "metadata": {},
   "source": [
    "## Styles\n",
    "\n",
    "Now let's visualize the structure. In the code below, notice that we first add a <i>model</i> and then add <i>styles</i>. The `view.addStyle` function requires an [atom selection](https://3dmol.csb.pitt.edu/doc/types.html#AtomSpec) and a [style specification](https://3dmol.csb.pitt.edu/doc/types.html#AtomStyleSpec). The types of styles include `line`, `stick`, `sphere`, and `cartoon` and properties of the styles include [colorscheme](https://3dmol.csb.pitt.edu/doc/types.html#ColorschemeSpec) or specific [color](https://3dmol.csb.pitt.edu/doc/types.html#ColorSpec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = py3Dmol.view()\n",
    "view.addModel(open('5RH2.pdb', 'r').read(),'pdb')\n",
    "view.setBackgroundColor('white')\n",
    "view.setStyle({'chain':'A'}, {'cartoon': {'color':'purple'}})\n",
    "view.addStyle({'resn':'UH7'}, {'stick': {'colorscheme':'yellowCarbon'}})\n",
    "view.addStyle({'within':{'distance':'5', 'sel':{'resn':'UH7'}}}, {'stick': {}})\n",
    "view.zoomTo()\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-surface",
   "metadata": {},
   "source": [
    "Try playing with the view by using the mouse controls\n",
    "\n",
    "| Movement | Mouse Input |\tTouch Input |\n",
    "| -------- | ----------- | -----------  |\n",
    "| Rotation |\tPrimary Mouse Button | Single touch |\n",
    "| Translation\t| Middle Mouse Button or Ctrl+Primary\t| Triple touch |\n",
    "| Zoom | Scroll Wheel or Second Mouse Button or Shift+Primary | Pinch (double touch) |\n",
    "| Slab |\tCtrl+Second |\tNot Available |\n",
    "\n",
    "Also, try to change the view by removing or modifying the styles.\n",
    "\n",
    "\n",
    "## Surfaces\n",
    "\n",
    "Now let's try adding a [surface](https://3dmol.csb.pitt.edu/doc/$3Dmol.GLViewer.html#addSurface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = py3Dmol.view()\n",
    "view.addModel(open('5RH2.pdb', 'r').read(),'pdb')\n",
    "view.setBackgroundColor('white')\n",
    "view.setStyle({'chain':'A'}, {'cartoon': {'color':'purple'}})\n",
    "view.addStyle({'resn':'UH7'}, {'stick': {'colorscheme':'yellowCarbon'}})\n",
    "view.addStyle({'within':{'distance':'5', 'sel':{'resn':'UH7'}}}, {'stick': {}})\n",
    "view.addSurface(py3Dmol.VDW, {'opacity':0.85, 'color':'grey'}, \\\n",
    "  {'not':{'or':[{'resn':'UH7'}, {'resn':'DMS'}]}})\n",
    "view.zoomTo()\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-suicide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python openfold",
   "language": "python",
   "name": "openfold-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
